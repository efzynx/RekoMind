{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAsf4YDilHHY",
        "outputId": "11e77783-7761-4ae1-c265-3661ee67f4a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Flask in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.11)\n",
            "Requirement already satisfied: mistralai in /usr/local/lib/python3.11/dist-packages (1.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n",
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.11/dist-packages (0.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: eval-type-backport>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.2.2)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.10.3 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.11.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from mistralai) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mistralai) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (from imblearn) (0.13.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->mistralai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.1->mistralai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10.3->mistralai) (2.33.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->mistralai) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.4.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.7)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.28.1->mistralai) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "# 1. INSTALL DEPENDENCIES\n",
        "!pip install Flask pyngrok mistralai scikit-learn pandas joblib imblearn sentence-transformers wikipedia xgboost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. KODE UTAMA (jalankan di sel kedua)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, learning_curve\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "import joblib\n",
        "import os\n",
        "import json\n",
        "from flask import Flask, request, jsonify\n",
        "import wikipedia\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from typing import List, Dict, Optional\n",
        "import re\n",
        "import html\n",
        "import urllib.parse\n",
        "import threading\n",
        "import time\n",
        "from pyngrok import ngrok, conf\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from mistralai import Mistral\n",
        "import textwrap\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "IXNF087DlUH7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Konfigurasi untuk Model 1\n",
        "MODEL_NAME_M1 = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
        "WIKIPEDIA_LANG = \"id\"\n",
        "NUM_SENTENCES_FOR_SUMMARY = 10\n",
        "NUM_ARTICLES_TO_FETCH_PER_QUERY = 3\n",
        "MIN_CONTENT_LENGTH_FOR_VALID_ARTICLE = 250\n",
        "SIMILARITY_THRESHOLD = 0.4\n",
        "\n",
        "# Konfigurasi untuk Model 2\n",
        "MODEL_FILE_M2 = 'user_understanding_classifier.joblib'\n",
        "DATA_FILE_M2 = '/content/rekomind_answer_history (4).csv'\n",
        "df_raw = pd.read_csv(DATA_FILE_M2)\n",
        "\n",
        "# Variabel global untuk model yang dimuat\n",
        "_sentence_model: Optional[SentenceTransformer] = None\n",
        "# _classifier_artifacts: Optional[dict] = None\n",
        "_classifier_artifacts: Dict = {}"
      ],
      "metadata": {
        "id": "l_GS1_mvmAjP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL 1 Rekomendasi Bacaan**"
      ],
      "metadata": {
        "id": "rfJ2PmX5mwSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _load_sentence_model_once_colab():\n",
        "    \"\"\"Memuat model Sentence Transformer sekali saja.\"\"\"\n",
        "    global _sentence_model\n",
        "    if _sentence_model is None:\n",
        "        try:\n",
        "            print(f\"MODEL 1: Memuat model Sentence Transformer: '{MODEL_NAME_M1}'...\")\n",
        "            _sentence_model = SentenceTransformer(MODEL_NAME_M1)\n",
        "            print(f\"MODEL 1: Model Sentence Transformer '{MODEL_NAME_M1}' berhasil dimuat.\")\n",
        "        except Exception as e:\n",
        "            print(f\"MODEL 1: GAGAL memuat model Sentence Transformer '{MODEL_NAME_M1}': {e}\")\n",
        "            _sentence_model = None\n",
        "    return _sentence_model"
      ],
      "metadata": {
        "id": "C7HqtMRPmcRF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _decode_and_clean_text_colab(text: str) -> str:\n",
        "    \"\"\"Membersihkan teks dari HTML, URL encoding, dan artefak lainnya.\"\"\"\n",
        "    if not text: return \"\"\n",
        "    decoded_text = text\n",
        "    decoded_text = urllib.parse.unquote(decoded_text)\n",
        "    decoded_text = html.unescape(decoded_text)\n",
        "    decoded_text = re.sub(r'==.*?==', '', decoded_text) # Hapus judul bagian wikipedia\n",
        "    decoded_text = re.sub(r'\\n', ' ', decoded_text) # Ganti baris baru dengan spasi\n",
        "    decoded_text = re.sub(r'\\s\\s+', ' ', decoded_text) # Hapus spasi berlebih\n",
        "    return decoded_text.strip()"
      ],
      "metadata": {
        "id": "HLj4slXvm2NN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _fetch_wikipedia_articles_for_search_keyword_colab(search_keyword: str) -> List[Dict]:\n",
        "    \"\"\"Mencari beberapa artikel relevan dari Wikipedia.\"\"\"\n",
        "    print(f\"  MODEL 1: Mencari artikel Wikipedia untuk keyword: '{search_keyword}'\")\n",
        "    wikipedia.set_lang(WIKIPEDIA_LANG)\n",
        "    articles_info = []\n",
        "    try:\n",
        "        search_results = wikipedia.search(search_keyword, results=NUM_ARTICLES_TO_FETCH_PER_QUERY)\n",
        "        if not search_results:\n",
        "            print(f\"    MODEL 1: Tidak ada hasil pencarian Wikipedia untuk '{search_keyword}'.\")\n",
        "            return []\n",
        "\n",
        "        for title in search_results:\n",
        "            try:\n",
        "                page = wikipedia.page(title, auto_suggest=True, redirect=True)\n",
        "                content = _decode_and_clean_text_colab(page.content)\n",
        "                if len(content) >= MIN_CONTENT_LENGTH_FOR_VALID_ARTICLE:\n",
        "                    articles_info.append({\n",
        "                        \"judul\": page.title, \"konten\": content, \"url\": page.url,\n",
        "                    })\n",
        "                    print(f\"    -> MODEL 1: Artikel '{page.title}' diproses (konten: {len(content)} chars).\")\n",
        "            except Exception as e_page:\n",
        "                print(f\"    MODEL 1 Error saat mengambil halaman '{title}': {e_page}\")\n",
        "    except Exception as e_search:\n",
        "        print(f\"  MODEL 1 Error saat wikipedia.search('{search_keyword}'): {e_search}\")\n",
        "    return articles_info"
      ],
      "metadata": {
        "id": "652d4_snm6Ps"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dynamic_content_recommendations_colab(query_text_for_embedding: str, search_keyword_for_wikipedia: str) -> List[Dict]:\n",
        "    \"\"\"Menghasilkan rekomendasi konten berdasarkan pencarian dan kemiripan.\"\"\"\n",
        "    if _sentence_model is None: return [{\"error\": \"Model AI (Sentence Transformer) Belum Siap\"}]\n",
        "\n",
        "    dynamic_corpus_articles = _fetch_wikipedia_articles_for_search_keyword_colab(search_keyword_for_wikipedia)\n",
        "    if not dynamic_corpus_articles: return []\n",
        "\n",
        "    all_sentences = []\n",
        "    article_map = []\n",
        "    for i, article in enumerate(dynamic_corpus_articles):\n",
        "        sentences = article['konten'].split('. ')\n",
        "        all_sentences.extend(sentences)\n",
        "        article_map.extend([i] * len(sentences))\n",
        "\n",
        "    query_embedding = _sentence_model.encode(query_text_for_embedding)\n",
        "    corpus_embeddings = _sentence_model.encode(all_sentences)\n",
        "    cosine_scores = util.cos_sim(query_embedding, corpus_embeddings)[0].numpy()\n",
        "\n",
        "    top_results_indices = np.argsort(-cosine_scores)[:NUM_SENTENCES_FOR_SUMMARY]\n",
        "\n",
        "    best_article_index = article_map[top_results_indices[0]]\n",
        "    best_article = dynamic_corpus_articles[best_article_index]\n",
        "\n",
        "    summary_sentences = [all_sentences[i] for i in top_results_indices if article_map[i] == best_article_index]\n",
        "    summary = '. '.join(summary_sentences)\n",
        "\n",
        "    if not summary: summary = all_sentences[top_results_indices[0]]\n",
        "\n",
        "    return [{\"judul\": best_article['judul'], \"summary\": summary, \"url\": best_article['url'], \"source\": \"Wikipedia (Dinamis)\"}]"
      ],
      "metadata": {
        "id": "U5Bxcw2Em8Iy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL 2 HYBRID (Klasifikasi + LLM)**"
      ],
      "metadata": {
        "id": "cKEPO-VInCpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Di bagian atas skrip Anda, setelah deklarasi _classifier_artifacts\n",
        "# _classifier_artifacts: Dict = {}\n",
        "\n",
        "def _load_classifier_artifacts_once_colab():\n",
        "    \"\"\"Memuat semua artefak yang diperlukan untuk Model 2 (Ensemble).\"\"\"\n",
        "    global _classifier_artifacts\n",
        "    if not _classifier_artifacts: # Hanya muat jika kamus kosong\n",
        "        try:\n",
        "            print(\"MODEL 2: Memuat artefak untuk model Ensemble...\")\n",
        "            # Sesuaikan path jika Anda menyimpannya di Google Drive\n",
        "            model_path = \"ensemble_model_reko.pkl\"\n",
        "            scaler_path = \"scaler_enhanced.pkl\"\n",
        "            encoder_path = \"category_encoder.pkl\"\n",
        "\n",
        "            _classifier_artifacts[\"model\"] = joblib.load(model_path)\n",
        "            _classifier_artifacts[\"scaler\"] = joblib.load(scaler_path)\n",
        "            _classifier_artifacts[\"category_encoder\"] = joblib.load(encoder_path)\n",
        "\n",
        "            print(\"✅ MODEL 2: Artefak Ensemble (model, scaler, encoder) berhasil dimuat.\")\n",
        "        except Exception as e:\n",
        "            print(f\"GAGAL memuat artefak Model 2: {e}\")\n",
        "            _classifier_artifacts = {} # Kosongkan jika gagal\n",
        "    return _classifier_artifacts"
      ],
      "metadata": {
        "id": "pCTCTbl5m-Ww"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_holistic_hybrid_analysis_from_data(performance_list: List[Dict]) -> str:\n",
        "    \"\"\"\n",
        "    Menganalisis performa pengguna menggunakan model Ensemble (RF+XGB+SVM)\n",
        "    dan menghasilkan ringkasan dengan LLM.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- [HYBRID MODEL V2] Memulai Analisis dengan Ensemble Model ---\")\n",
        "\n",
        "    if not _classifier_artifacts or \"model\" not in _classifier_artifacts:\n",
        "        return \"Error: Model klasifikasi (Ensemble) belum siap.\"\n",
        "    if not performance_list:\n",
        "        return \"Tidak ada data performa yang diterima.\"\n",
        "\n",
        "    try:\n",
        "        df = pd.DataFrame(performance_list)\n",
        "\n",
        "        # --- LANGKAH 1: REPLIKASI FEATURE ENGINEERING ---\n",
        "        # Pastikan kolom-kolom yang diperlukan ada\n",
        "        required_cols = {'category_name', 'total_questions', 'correct_count', 'difficulty_score'}\n",
        "        if not required_cols.issubset(df.columns):\n",
        "             # Buat kolom dummy jika tidak ada, meskipun idealnya data input harus lengkap\n",
        "            if 'difficulty_score' not in df.columns: df['difficulty_score'] = 2.0\n",
        "            if 'correct_count' not in df.columns: df['correct_count'] = df['accuracy'] * df['total_questions']\n",
        "\n",
        "\n",
        "        # Buat fitur-fitur baru persis seperti saat training\n",
        "        df['correct_ratio'] = df['correct_count'] / df['total_questions']\n",
        "        df['interaction'] = df['total_questions'] * df['difficulty_score']\n",
        "        df['difficulty_correct_ratio'] = df['difficulty_score'] * df['correct_ratio']\n",
        "\n",
        "        # Ganti NaN yang mungkin muncul dari pembagian dengan 0\n",
        "        df.fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "        # --- LANGKAH 2: PREPROCESSING UNTUK PREDIKSI ---\n",
        "        cat_encoder = _classifier_artifacts[\"category_encoder\"]\n",
        "        scaler = _classifier_artifacts[\"scaler\"]\n",
        "        model = _classifier_artifacts[\"model\"]\n",
        "\n",
        "        # Transformasi kategori, handle kategori yang belum pernah dilihat\n",
        "        df[\"category_name\"] = df[\"category_name\"].apply(lambda x: cat_encoder.transform([x])[0] if x in cat_encoder.classes_ else -1)\n",
        "\n",
        "        # Filter baris dengan kategori yang tidak dikenal\n",
        "        df_predictable = df[df[\"category_name\"] != -1].copy()\n",
        "\n",
        "        if df_predictable.empty:\n",
        "            return \"Tidak ada data kategori yang bisa diprediksi.\"\n",
        "\n",
        "        # Pilih fitur yang sama dengan yang digunakan saat training\n",
        "        features = [\n",
        "            \"category_name\", \"total_questions\", \"difficulty_score\",\n",
        "            \"correct_ratio\", \"interaction\", \"difficulty_correct_ratio\"\n",
        "        ]\n",
        "        # Ganti nama kolom 'category_name' menjadi 'category_name' untuk konsistensi dengan scaler\n",
        "        df_predictable.rename(columns={'category_name': 'category_name'}, inplace=True)\n",
        "\n",
        "        X = df_predictable[features]\n",
        "        X_scaled = scaler.transform(X)\n",
        "\n",
        "        # --- LANGKAH 3: PREDIKSI DAN PERSIAPAN PROMPT ---\n",
        "        df_predictable[\"model_prediction\"] = model.predict(X_scaled)\n",
        "\n",
        "        # Ganti kembali nama kolom 'category_name' ke nama aslinya untuk output\n",
        "        df_predictable['category_name_str'] = cat_encoder.inverse_transform(df_predictable['category_name'])\n",
        "\n",
        "\n",
        "        # Siapkan data untuk prompt LLM\n",
        "        performance_summary = df_predictable[[\"category_name_str\", \"accuracy\", \"model_prediction\"]].to_dict(\"records\")\n",
        "        for record in performance_summary:\n",
        "            record['category_name'] = record.pop('category_name_str') # Ganti nama kunci\n",
        "\n",
        "        # Soal yang salah (logika ini bisa tetap sama)\n",
        "        wrong_answers_list = []\n",
        "        for item in performance_list:\n",
        "            contoh_salah = item.get(\"example_incorrect_questions\", [])\n",
        "            for q in contoh_salah:\n",
        "                wrong_answers_list.append({\n",
        "                    \"category_name\": item[\"category_name\"],\n",
        "                    \"question_text\": q.get(\"question_text\", \"\"),\n",
        "                    \"user_answer\": q.get(\"user_answer\", \"\"),\n",
        "                    \"correct_answer\": q.get(\"correct_answer\", \"\")\n",
        "                })\n",
        "\n",
        "        soal_salah_section = (\n",
        "            \"\\n## ❌ Contoh Soal yang Salah:\\n```json\\n\" + json.dumps(wrong_answers_list, indent=2) + \"\\n```\"\n",
        "            if wrong_answers_list else \"\"\n",
        "        )\n",
        "\n",
        "        # --- LANGKAH 4: PANGGIL LLM ---\n",
        "        # Prompt untuk Mistral (bisa tetap sama)\n",
        "        prompt = (\n",
        "            'Kamu adalah \"Reko\", asisten AI pembelajaran yang cerdas, suportif, dan to the point.\\n'\n",
        "            'Tugasmu: bantu pengguna memahami kekuatan dan kelemahan belajar mereka berdasarkan data statistik dan prediksi model, serta contoh soal yang mereka jawab salah.\\n\\n'\n",
        "            '## 📊 Ringkasan Performa + Prediksi Model Ensemble (Canggih):\\n'\n",
        "            '```json\\n' + json.dumps(performance_summary, indent=2) + '\\n```\\n'\n",
        "            f'{soal_salah_section}\\n\\n'\n",
        "            '🎯 Tugasmu:\\n'\n",
        "            '- Sampaikan analisis menyeluruh tapi singkat dan natural berdasarkan data di atas.\\n'\n",
        "            '- Bahas 1-2 soal yang dijawab salah (jika ada) dan beri saran cara belajar yang relevan.\\n'\n",
        "            '- Berikan insight yang actionable dan spesifik berdasarkan kategori dan tingkat pemahaman yang diprediksi.\\n'\n",
        "            '- Tampilkan tabel markdown ringkas dan akhiri dengan motivasi ringan yang positif.'\n",
        "        )\n",
        "\n",
        "        # Kirim ke LLM (logika ini bisa tetap sama)\n",
        "        api_key = os.environ.get(\"MISTRAL_API_KEY\", \"NsG05vqdKwNxJLnKw4SQW576qTf6YUbH\") # Ganti dengan API key Anda jika perlu\n",
        "        client = Mistral(api_key=api_key)\n",
        "        response = client.chat.complete(\n",
        "            model=\"mistral-small-latest\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(f\"FATAL HYBRID ERROR: {e}\\n{traceback.format_exc()}\")\n",
        "        return f\"Terjadi error saat melakukan analisis hybrid: {e}\""
      ],
      "metadata": {
        "id": "zWxGoDbkog2Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ROUTING Flask + NGROK**"
      ],
      "metadata": {
        "id": "cvjy3EagoscT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ENDPOINT FLASK ---\n",
        "app = Flask(__name__)"
      ],
      "metadata": {
        "id": "XUGCk8TQorVr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.route('/generate_recommendations_v2', methods=['POST'])\n",
        "def generate_recommendations_endpoint_v2():\n",
        "    \"\"\"Endpoint untuk Model 1, sesuai dengan 'rekomendaimodels.py'.\"\"\"\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        if not data or \"query_text_for_embedding\" not in data or \"search_keyword_for_wikipedia\" not in data:\n",
        "            return jsonify({\"error\": \"query_text_for_embedding dan search_keyword_for_wikipedia wajib\"}), 400\n",
        "\n",
        "        query_embed = data[\"query_text_for_embedding\"]\n",
        "        search_wiki = data[\"search_keyword_for_wikipedia\"]\n",
        "\n",
        "        recommendations = get_dynamic_content_recommendations_colab(query_embed, search_wiki)\n",
        "        return jsonify(recommendations)\n",
        "    except Exception as e:\n",
        "        print(f\"API Error di /generate_recommendations_v2: {e}\")\n",
        "        return jsonify({\"error\": \"Internal server error\", \"details\": str(e)}), 500"
      ],
      "metadata": {
        "id": "KLuCFSpTo0rr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@app.route('/holistic_hybrid_analysis', methods=['POST'])\n",
        "def handle_holistic_hybrid_analysis():\n",
        "    \"\"\"\n",
        "    Endpoint utama dan satu-satunya untuk Model 2.\n",
        "    Menerima 'performance_data' dari aplikasi lokal.\n",
        "    \"\"\"\n",
        "    print(\"COLAB SERVICE: Menerima permintaan untuk analisis hybrid...\")\n",
        "    data = request.json\n",
        "    performance_list = data.get('performance_data')\n",
        "    if not performance_list:\n",
        "        return jsonify({\"error\": \"Payload 'performance_data' dibutuhkan.\"}), 400\n",
        "\n",
        "    summary_text = get_holistic_hybrid_analysis_from_data(performance_list)\n",
        "    return jsonify({\"holistic_analysis\": summary_text})\n"
      ],
      "metadata": {
        "id": "2_mxbiOxo1Mn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    print(\"--- Menyiapkan Layanan AI Rekomind di Colab ---\")\n",
        "    # Muat Model 1\n",
        "    _load_sentence_model_once_colab()\n",
        "\n",
        "    # Muat Model 2 (Ensemble)\n",
        "    _load_classifier_artifacts_once_colab()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHUF7up1o6F9",
        "outputId": "ff96add2-89a5-44cf-b56f-7aeff8beec4e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Menyiapkan Layanan AI Rekomind di Colab ---\n",
            "MODEL 1: Memuat model Sentence Transformer: 'paraphrase-multilingual-MiniLM-L12-v2'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL 1: Model Sentence Transformer 'paraphrase-multilingual-MiniLM-L12-v2' berhasil dimuat.\n",
            "MODEL 2: Memuat artefak untuk model Ensemble...\n",
            "✅ MODEL 2: Artefak Ensemble (model, scaler, encoder) berhasil dimuat.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    def run_flask_app():\n",
        "        # Menjalankan app Flask tanpa debug mode untuk produksi di thread\n",
        "        app.run(host=\"0.0.0.0\", port=5000, debug=False)\n",
        "\n",
        "    flask_thread = threading.Thread(target=run_flask_app)\n",
        "    flask_thread.daemon = True\n",
        "    flask_thread.start()\n"
      ],
      "metadata": {
        "id": "K6wefNDOo_gi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Setup ngrok\n",
        "    NGROK_AUTH_TOKEN = os.environ.get(\"NGROK_AUTH_TOKEN\", \"1wIKVgVrfDvzbOpqvTYd84P3qTo_649KyUcCJXPYtKcBbygkh\")\n",
        "    if NGROK_AUTH_TOKEN:\n",
        "        conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
        "    else:\n",
        "        print(\"\\n!!! PERINGATAN: NGROK_AUTH_TOKEN tidak diset di 'Secrets' Colab. Tunnel mungkin memiliki batasan waktu. !!!\")\n",
        "\n",
        "    public_url_global = None\n",
        "    try:\n",
        "        # Menutup tunnel yang mungkin sudah ada\n",
        "        for tunnel in ngrok.get_tunnels():\n",
        "            ngrok.disconnect(tunnel.public_url)\n",
        "\n",
        "        # Membuat tunnel baru\n",
        "        public_url_global = ngrok.connect(5000, \"http\", bind_tls=True)\n",
        "        print(\"\\n--- Aplikasi Siap Menerima Permintaan ---\")\n",
        "        print(\"-\" * 50)\n",
        "        print(f\" * Ngrok Tunnel (HTTPS) Aktif di: {public_url_global}\")\n",
        "        print(f\" * Endpoint Model 1: {public_url_global}/generate_recommendations_v2\")\n",
        "        print(f\" * Endpoint Model 2 Hybrid: {public_url_global}/holistic_hybrid_analysis\")\n",
        "        print(\" * API Rekomendasi di Colab SIAP DIAKSES.\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Menjaga script tetap berjalan\n",
        "        while True:\n",
        "            time.sleep(3600)\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nCOLAB: Proses dihentikan oleh pengguna.\")\n",
        "    except Exception as e_ngrok:\n",
        "        print(f\"Error memulai Ngrok: {e_ngrok}\")\n",
        "    finally:\n",
        "        print(\"COLAB: Menutup tunnel Ngrok...\")\n",
        "        if public_url_global:\n",
        "            ngrok.disconnect(public_url_global)\n",
        "        ngrok.kill()\n",
        "        print(\"COLAB: Selesai.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S37Gd_A_pH1P",
        "outputId": "7618a748-14dc-4eb7-f859-c136c643c312"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Aplikasi Siap Menerima Permintaan ---\n",
            "--------------------------------------------------\n",
            " * Ngrok Tunnel (HTTPS) Aktif di: NgrokTunnel: \"https://e7c1-35-227-20-103.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Endpoint Model 1: NgrokTunnel: \"https://e7c1-35-227-20-103.ngrok-free.app\" -> \"http://localhost:5000\"/generate_recommendations_v2\n",
            " * Endpoint Model 2 Hybrid: NgrokTunnel: \"https://e7c1-35-227-20-103.ngrok-free.app\" -> \"http://localhost:5000\"/holistic_hybrid_analysis\n",
            " * API Rekomendasi di Colab SIAP DIAKSES.\n",
            "--------------------------------------------------\n",
            "COLAB SERVICE: Menerima permintaan untuk analisis hybrid...\n",
            "\n",
            "--- [HYBRID MODEL V2] Memulai Analisis dengan Ensemble Model ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [04/Jul/2025 09:06:58] \"POST /holistic_hybrid_analysis HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  MODEL 1: Mencari artikel Wikipedia untuk keyword: 'Bo Jackson which these celebrities Celebrities'\n",
            "    -> MODEL 1: Artikel 'Janet Jackson' diproses (konten: 13301 chars).\n",
            "    -> MODEL 1: Artikel 'Daftar penghargaan dan nominasi yang diterima oleh BTS' diproses (konten: 2091 chars).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [04/Jul/2025 09:12:05] \"POST /generate_recommendations_v2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  MODEL 1: Mencari artikel Wikipedia untuk keyword: 'Jon Jafari which these people Celebrities'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [04/Jul/2025 09:12:06] \"POST /generate_recommendations_v2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    MODEL 1: Tidak ada hasil pencarian Wikipedia untuk 'Jon Jafari which these people Celebrities'.\n",
            "  MODEL 1: Mencari artikel Wikipedia untuk keyword: 'Pneumonia what was the Celebrities'\n",
            "    -> MODEL 1: Artikel 'George H. W. Bush' diproses (konten: 27841 chars).\n",
            "    -> MODEL 1: Artikel 'Marlon Brando' diproses (konten: 95265 chars).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [04/Jul/2025 09:12:41] \"POST /generate_recommendations_v2 HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COLAB SERVICE: Menerima permintaan untuk analisis hybrid...\n",
            "\n",
            "--- [HYBRID MODEL V2] Memulai Analisis dengan Ensemble Model ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [04/Jul/2025 09:12:52] \"POST /holistic_hybrid_analysis HTTP/1.1\" 200 -\n",
            "WARNING:pyngrok.process.ngrok:t=2025-07-04T09:16:25+0000 lvl=warn msg=\"Stopping forwarder\" name=http-5000-4129e8d6-f545-4397-87dd-400cfe4f1b0d acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "COLAB: Proses dihentikan oleh pengguna.\n",
            "COLAB: Menutup tunnel Ngrok...\n",
            "COLAB: Selesai.\n"
          ]
        }
      ]
    }
  ]
}