# -*- coding: utf-8 -*-
"""RF_Reko_XGBoost1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c2A0ZidZN2n--YYsLktz0ik8LqyJc46d
"""

# Install library yang diperlukan
!pip install scikit-learn pandas joblib imblearn xgboost

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, learning_curve
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, balanced_accuracy_score, f1_score
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
import joblib
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Muat dataset mentah
DATA_FILE_M2 = '/content/rekomind_answer_history (4).csv'
df_raw = pd.read_csv(DATA_FILE_M2)
print("‚úÖ Data mentah dimuat. Jumlah data:", len(df_raw))

# cek duplikasi data
duplikat_soal = df_raw[df_raw.duplicated(subset=["question_text", "correct_answer", "category_name"], keep=False)]
print("üîÅ Jumlah pertanyaan berulang:", len(duplikat_soal))

# hapus duplikasi data
df_dedup = df_raw.drop_duplicates(
subset=["question_text", "correct_answer", "category_name"],
keep='first'
)
print(f"‚úÖ Data setelah hapus duplikat: {len(df_dedup)}")

# cek duplikasi data
duplikat_soal = df_dedup[df_dedup.duplicated(subset=["question_text", "correct_answer", "category_name"], keep=False)]
print("üîÅ Jumlah pertanyaan berulang:", len(duplikat_soal))

# Buat fitur numerik untuk kesulitan
difficulty_map = {'easy': 1.0, 'medium': 2.0, 'hard': 3.0}
df_dedup['difficulty_score'] = df_raw['difficulty'].map(difficulty_map).fillna(2.0)

# Agregasi per user + kategori
df_grouped = df_dedup.groupby(['user_id', 'category_name']).agg(
    total_questions=('is_correct', 'count'),
    correct_count=('is_correct', 'sum'),
    incorrect_count=('is_correct', lambda x: (~x).sum()),
    accuracy=('is_correct', 'mean'),
    difficulty_score=('difficulty_score', 'mean')
).reset_index()

# Buat label/target
def classify_accuracy(acc):
    if acc >= 0.8:
        return "Kuat"
    elif acc >= 0.6:
        return "Cukup"
    else:
        return "Lemah"

df_grouped['tingkat_pemahaman'] = df_grouped['accuracy'].apply(classify_accuracy)

# Drop user_id
df_grouped = df_grouped.drop(columns=['user_id'])

df_grouped.head()

df_grouped.info()

# Feature Engineering: Tambahkan fitur baru
df_grouped['correct_ratio'] = df_grouped['correct_count'] / df_grouped['total_questions']
df_grouped['interaction'] = df_grouped['total_questions'] * df_grouped['difficulty_score']
df_grouped['difficulty_correct_ratio'] = df_grouped['difficulty_score'] * df_grouped['correct_ratio']

# Ubah kolom kategori jadi string
df_grouped['category_name'] = df_grouped['category_name'].astype(str)

# Encoding kolom kategori
cat_encoder = LabelEncoder()
df_grouped["category_name"] = cat_encoder.fit_transform(df_grouped["category_name"])

# Distribusi kelas
print("\nüìä Distribusi kelas awal:")
print(df_grouped['tingkat_pemahaman'].value_counts())

# Pisahkan fitur dan target
X = df_grouped[["category_name", "total_questions", "difficulty_score",
                    "correct_ratio", "interaction", "difficulty_correct_ratio"]]
y = df_grouped["tingkat_pemahaman"]

# Hitung class weights
class_counts = y.value_counts()
total_samples = len(y)
class_weights = {
    'Lemah': total_samples / (3 * class_counts['Lemah']),
    'Cukup': total_samples / (3 * class_counts['Cukup']),
    'Kuat': total_samples / (3 * class_counts['Kuat'])
}

print("\n‚öñÔ∏è Class weights yang dihitung:")
for cls, weight in class_weights.items():
    print(f"{cls}: {weight:.2f}")

# Split data dengan stratify
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

# Terapkan SMOTE hanya pada data training
print("\nüîÅ Menerapkan SMOTE untuk menyeimbangkan kelas...")
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

print("Distribusi kelas setelah SMOTE:")
print(y_train_res.value_counts())

# Scaling data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_res)
X_test_scaled = scaler.transform(X_test)

# 1. Hyperparameter Tuning untuk Random Forest
print("\nüîé Melakukan Hyperparameter Tuning untuk Random Forest...")

# Buat mapping kelas ke integer
class_mapping = {'Cukup': 0, 'Kuat': 1, 'Lemah': 2}
y_train_numeric = y_train_res.map(class_mapping)

# PERBAIKAN: Gunakan class_weight='balanced' untuk menghindari masalah format
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'class_weight': ['balanced', 'balanced_subsample']  # Gunakan opsi built-in
}

rf = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    scoring='f1_macro',
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train_scaled, y_train_numeric)

print("\nüéØ Best Parameters for Random Forest:")
print(grid_search.best_params_)
print("Best F1 Macro Score:", grid_search.best_score_)

best_rf = grid_search.best_estimator_

# 2. XGBoost dengan Bobot Kelas
print("\nüöÄ Melatih XGBoost dengan Bobot Kelas...")
# Hitung scale_pos_weight untuk XGBoost
scale_weights = {
    'Cukup': class_counts['Lemah']/class_counts['Cukup'],
    'Kuat': class_counts['Lemah']/class_counts['Kuat'],
    'Lemah': 1.0
}

y_test_numeric = y_test.map(class_mapping)

xgb = XGBClassifier(
    objective='multi:softmax',
    num_class=3,
    scale_pos_weight=[scale_weights['Cukup'], scale_weights['Kuat'], scale_weights['Lemah']],
    random_state=42,
    n_estimators=200,
    learning_rate=0.1,
    max_depth=5,
    subsample=0.8,
    colsample_bytree=0.8
)

xgb.fit(X_train_scaled, y_train_numeric)

# 3. Ensemble Model dengan Voting Classifier
print("\nü§ù Membuat Ensemble Model...")

# Gunakan model Random Forest terbaik dengan class_weight built-in
# SVM dengan bobot kelas 'balanced'
svm = SVC(
    probability=True,
    class_weight='balanced',  # Gunakan balanced untuk menghindari masalah
    random_state=42,
    kernel='rbf',
    C=1.0,
    gamma='scale'
)

# Buat ensemble model
ensemble = VotingClassifier(
    estimators=[
        ('rf', best_rf),
        ('xgb', xgb),
        ('svm', svm)
    ],
    voting='soft',  # Gunakan soft voting untuk probabilitas
    n_jobs=-1
)

# Latih ensemble dengan target string asli
ensemble.fit(X_train_scaled, y_train_res)

# 4. Evaluasi Ensemble Model
print("\nüìä Evaluasi Ensemble Model:")

# Prediksi
y_pred = ensemble.predict(X_test_scaled)

print("\n===== Ensemble Model Performance =====")
print("‚úÖ Akurasi:", accuracy_score(y_test, y_pred))
print("‚úÖ Balanced Accuracy:", balanced_accuracy_score(y_test, y_pred))
print("‚úÖ F1 Score (macro):", f1_score(y_test, y_pred, average='macro'))
print("\nüìä Classification Report:\n", classification_report(y_test, y_pred))
print("üìâ Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

print("\n===== XGBoost Performance =====")
xgb_pred = xgb.predict(X_test_scaled)
xgb_pred_labels = [list(class_mapping.keys())[list(class_mapping.values()).index(val)] for val in xgb_pred]

print("‚úÖ Akurasi:", accuracy_score(y_test, xgb_pred_labels))
print("‚úÖ Balanced Accuracy:", balanced_accuracy_score(y_test, xgb_pred_labels))
print("‚úÖ F1 Score (macro):", f1_score(y_test, xgb_pred_labels, average='macro'))
print("\nüìä Classification Report:\n", classification_report(y_test, xgb_pred_labels))

# Visualisasi Confusion Matrix untuk Ensemble
plt.figure(figsize=(10, 8))
plt.subplot(2, 1, 1)
sns.heatmap(confusion_matrix(y_test, y_pred),
            annot=True, fmt="d", cmap="rocket",
            xticklabels=["Cukup", "Kuat", "Lemah"],
            yticklabels=["Cukup", "Kuat", "Lemah"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Ensemble Model")

# Visualisasi Confusion Matrix untuk XGBoost
plt.subplot(2, 1, 2)
sns.heatmap(confusion_matrix(y_test, xgb_pred_labels),
            annot=True, fmt="d", cmap="viridis",
            xticklabels=["Cukup", "Kuat", "Lemah"],
            yticklabels=["Cukup", "Kuat", "Lemah"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - XGBoost")

plt.tight_layout()
plt.show()

# Simpan model dan komponen
joblib.dump(ensemble, "ensemble_model_reko.pkl")
joblib.dump(xgb, "xgb_model_reko.pkl")
joblib.dump(scaler, "scaler_enhanced.pkl")
joblib.dump(cat_encoder, "category_encoder.pkl")
joblib.dump(class_mapping, "class_mapping.pkl")  # Simpan mapping kelas

print("\nüíæ Model dan komponen berhasil disimpan:")
print("- ensemble_model_reko.pkl (Voting Classifier)")
print("- xgb_model_reko.pkl (XGBoost Model)")
print("- scaler_enhanced.pkl")
print("- category_encoder.pkl")
print("- class_mapping.pkl (Class mapping)")

# Feature Importance dari XGBoost
plt.figure(figsize=(10, 6))
feature_importances = pd.Series(xgb.feature_importances_, index=X.columns)
feature_importances.nlargest(10).plot(kind='barh')
plt.title('Feature Importance - XGBoost')
plt.xlabel('Importance Score')
plt.ylabel('Features')
plt.show()

# Feature Importance dari Random Forest
rf_importances = pd.Series(best_rf.feature_importances_, index=X.columns)
plt.figure(figsize=(10, 6))
rf_importances.nlargest(10).plot(kind='barh', color='green')
plt.title('Feature Importance - Random Forest')
plt.xlabel('Importance Score')
plt.ylabel('Features')
plt.show()